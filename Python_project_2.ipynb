{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_project_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "742wsQ5R98Jz"
      },
      "source": [
        "# \"Война и Мир\"\r\n",
        "\r\n",
        "Сегодня мы обратимся к классике: применим анализ данных к «Войне и миру» Льва Толстого!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McnDu-ue__k_"
      },
      "source": [
        "Текст произведения мы взяли в библиотеке **lib.ru** и провели первоначальную обработку. Поскольку наша цель — обработка слов из этого произведения, мы разбили текст на слова и вывели каждое слово в отдельной строке. Кроме того, в местах, где начинаются главы, мы вывели строку [new chapter]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRvUi__IAgiB"
      },
      "source": [
        "# Задание 1\r\n",
        "\r\n",
        "Входные данные: строка `target_word`.\r\n",
        "Итак, начнём! Для начала посчитаем частоту использования отдельных слов в произведении. Для решения задачи воспользуемся уже знакомым нам словарём.\r\n",
        "\r\n",
        "Задание. Напишите программу, которая переберет все слова и занесет их в словарь (назвать его можете как угодно). Увеличивайте счётчик при добавлении каждого нового слова, чтобы посчитать сколько раз это слово встречается в тексте.\r\n",
        "\r\n",
        "Подсказка. Напоминаем, что метод `get` поможет проверить, какое значение соответствует ключу (слову) в словаре. Например, `words.get(word, 0)` вернёт либо значение из словаря, либо , если соответствующее слово пока отсутствует.\r\n",
        "\r\n",
        "Не забудьте добавить в начало своего кода функцию для чтения текста произведения:\r\n",
        "В качестве результата выведите, сколько раз слово `target_word` было найдено в тексте. Например, для `target_word = 'князь'` ответ будет `1289`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OyHclgq95RG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9877727-9bc4-46b7-ac00-50825188e13f"
      },
      "source": [
        "from collections import Counter\r\n",
        "\r\n",
        "#загружаю весь текст и разбиваю его на слова\r\n",
        "#Функция вернёт массив всех слов по порядку, а в местах начала новых глав будет строка [new chapter].\r\n",
        "def read_data():\r\n",
        "    data = open('data/war_peace_processed.txt', 'rt').read()\r\n",
        "    return data.split('\\n')\r\n",
        "\r\n",
        "data = read_data()\r\n",
        "\r\n",
        "new_dict = Counter(read_data())\r\n",
        "\r\n",
        "#Проверка результата\r\n",
        "target_word = 'князь'\r\n",
        "print(new_dict.get(target_word, 0))\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svip6smqB7rI"
      },
      "source": [
        "# Задание 2\r\n",
        "\r\n",
        "Входные данные: строка `target_word`.\r\n",
        "\r\n",
        "Пришло время познакомиться с понятием *document frequency*.\r\n",
        "\r\n",
        "*Document frequency* (для удобства сократим до *df*) — это доля документов, в которых встречается искомое слово. В нашем случае речь идёт не о документах, а о главах книги (выше мы писали, что в текстовом документе главы разделяются строкой `[new chapter]` ).\r\n",
        "\r\n",
        "**Задание.** Напишите программу, которая посчитает *document frequency* для заданного слова `target_word` и выведите результат на экран.\r\n",
        "\r\n",
        "*Подсказка*. Вычислить df можно по формуле:\r\n",
        "\r\n",
        "`number_of_documents_with_target_word / number_of_documents`\r\n",
        "* `number_of_documents` — общее количество глав\r\n",
        "* `number_of_documents_with_target_word` — количество глав, в которых встречается target_word\r\n",
        "\r\n",
        "Объясним на примере: наш текст состоит из  главы (`number_of_documents`), а слово человек встречается в  главах (`number_of_documents_with_target_word`).\r\n",
        "\r\n",
        "*df* слова человек = `115/171 = 0.672514619883041`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh_PUbWTBtSi",
        "outputId": "60c76f60-956e-4d34-8c0d-37c6afaf2c5d"
      },
      "source": [
        "def read_data():\r\n",
        "    data = open('data/war_peace_processed.txt', 'rt').read()\r\n",
        "    return data.split('\\n')\r\n",
        "data = read_data()\r\n",
        "\r\n",
        "index_new_chapter = [i for i, n in enumerate(data) if n == \"[new chapter]\"] \r\n",
        "\r\n",
        "# Create dictionary, where text by chapters\r\n",
        "c = 0\r\n",
        "dictionary_chapter = {}\r\n",
        "for a, b in enumerate(index_new_chapter):\r\n",
        "    dictionary_chapter[a+1] = data[c:b]\r\n",
        "    c = b\r\n",
        "dictionary_chapter[171] = data[299557:300079] \r\n",
        "\r\n",
        "# Count intances of target_word\r\n",
        "def count_instances(target_word):\r\n",
        "    count_1 = 0\r\n",
        "    i = 1\r\n",
        "    while i <= len(dictionary_chapter):\r\n",
        "        if target_word in dictionary_chapter[i]:\r\n",
        "            count_1 += 1\r\n",
        "        i +=1\r\n",
        "    return count_1\r\n",
        "\r\n",
        "# Count document frequency\r\n",
        "number_of_documents = count_instances(target_word)\r\n",
        "number_of_documents_with_target_word = len(dictionary_chapter)\r\n",
        "df = number_of_documents / number_of_documents_with_target_word\r\n",
        "\r\n",
        "#Проверка результата\r\n",
        "target_word = 'человек'\r\n",
        "print(df)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.672514619883041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYICf3-gEVYh"
      },
      "source": [
        "# Задание 3\r\n",
        "\r\n",
        "Сделаем следущий шаг: посчитаем частоту употребления отдельного слова в документе (`term frequency`, или `tf`).\r\n",
        "\r\n",
        "Проще всего объяснить, что такое `term frequency`, на примере:\r\n",
        "`tf` слова *война = количество раз, когда слово война встречается в тексте главы / количество всех слов в тексте главы*\r\n",
        "\r\n",
        "Попробуем посчитать частоту употребления слова `гостья` в `15`-й главе (кстати, у нас главы нумеруются с `0`). Слово гостья встречается в `15`-й главе `10` раз, а общее количество слов — `1359`.\r\n",
        "\r\n",
        "tf слова `гостья` в `15` главе = `101359` = `0.007358351729212656`.\r\n",
        "\r\n",
        "Задание. Напишите программу, которая выведет частоту употребления заданного слова `target_word` в заданной главе `target_chapter`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH_hmTmcDIfj",
        "outputId": "47ecc7da-18b3-4be3-b35b-4e200fa2eae6"
      },
      "source": [
        "def read_data():\r\n",
        "    data = open('data/war_peace_processed.txt', 'rt').read()\r\n",
        "    return data.split('\\n')\r\n",
        "\r\n",
        "data = read_data()\r\n",
        "\r\n",
        "index_new_chapter = [i for i, n in enumerate(data) if n == \"[new chapter]\"] \r\n",
        "\r\n",
        "# Create dictionary, where text by chapters\r\n",
        "c = 0\r\n",
        "dictionary_chapter = {}\r\n",
        "for a, b in enumerate(index_new_chapter):\r\n",
        "    dictionary_chapter[a] = data[c:b]\r\n",
        "    c = b\r\n",
        "dictionary_chapter[170] = data[299557:300079] \r\n",
        "\r\n",
        "\r\n",
        "def calculator_tf(target_word, target_chapter):\r\n",
        "    target_word_counter = 0\r\n",
        "    for i in dictionary_chapter[target_chapter]:\r\n",
        "        if i == target_word:\r\n",
        "            target_word_counter +=1\r\n",
        "    \r\n",
        "    total_words_in_chapter = len(dictionary_chapter[target_chapter])-1\r\n",
        "    return target_word_counter / total_words_in_chapter\r\n",
        "\r\n",
        "#Проверка результата\r\n",
        "target_word = 'гостья'\r\n",
        "target_chapter = 15\r\n",
        "print(calculator_tf(target_word, target_chapter))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.007358351729212656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO_aezECGbae"
      },
      "source": [
        "# Задание 4\r\n",
        "\r\n",
        "Входные данные:\r\n",
        "* строка `target_word`\r\n",
        "* число `target_chapter`\r\n",
        "\r\n",
        "Пришло время дать разъяснения: для чего мы делали вычисления выше и что нас ждет впереди?\r\n",
        "\r\n",
        "Если какое-то слово часто употребляется в документе, то, вероятно, этот документ что-то рассказывает о предмете/действии, описываемом этим словом. Скажем, если вы читаете книгу, в которой много раз употребляется слово *заяц*, то, вероятно, эта книга про зайцев.\r\n",
        "\r\n",
        "Однако, если вы возьмёте слово *и*, то оно будет встречаться почти в каждой книге много раз. Таким образом, если мы хотим найти наиболее значимые слова в книге, мы, с одной стороны, хотим найти наиболее частые слова, а с другой — убрать те, которые не несут важной информации, так как встречаются везде.\r\n",
        "\r\n",
        "**Подсказка**. Такая задача хорошо решается с помощью `tf*idf` — статистической метрики для оценки важности слова в тексте. Другими словами, `tf*idf` — это «контрастность» слова в документе (насколько оно выделяется среди других слов).\r\n",
        "\r\n",
        "`tf*idf = term frequency * inverse document frequency`\r\n",
        "* `tf` — это частотность термина, которая измеряет, насколько часто термин встречается в документе.\r\n",
        "* `idf` — это обратная документная частотность термина. Она измеряет непосредственно важность термина во всём множестве документов.\r\n",
        "\r\n",
        "Чтобы получить `idf`, необходимо поделить  на полученную в **Задании 2** документную частоту (`df`).\r\n",
        "\r\n",
        "Мы будем использовать не сырые значения tf и idf, а их логарифмы, то есть `log(1+tf) * log(idf)`. Сейчас мы не будем заострять внимания на том, почему следует использовать именно логарифмы — это долгий разговор. Поговорим об этом в курсе Математика и алгоритмы для машинного обучения.\r\n",
        "\r\n",
        "В качестве примера измерим `tf*idf`   слова `анна` в главе `4`. Слово *анна* встречается в указанной главе `7` раз (tf), при этом в `4` главе `1060` слов (`chapter_size`), всего же слово анна упоминается в  главах (df) из  (`chapter_count`).\r\n",
        "\r\n",
        "Таким образом, tf*idf данного слова в данной главе будет равно `math.log(1+tf/chapter_size) * math.log(1 / df)`, то есть `0.011031063403921838`\r\n",
        "\r\n",
        "Задание. Напишите программу, которая выведет значение `tf*idf` для заданного слова `target_word` в заданной главе `target_chapter`.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdzPgJsHFdK7",
        "outputId": "7fde1fdd-1f0c-4b95-c7cc-14c2cbf20626"
      },
      "source": [
        "import math\r\n",
        "\r\n",
        "def read_data():\r\n",
        "    data = open('data/war_peace_processed.txt', 'rt').read()\r\n",
        "    return data.split('\\n')\r\n",
        "\r\n",
        "data = read_data()\r\n",
        "# напишите ваш код ниже\r\n",
        "import math\r\n",
        "index_new_chapter = [i for i, n in enumerate(data) if n == \"[new chapter]\"] \r\n",
        "\r\n",
        "# Create dictionary, where text by chapters\r\n",
        "c = 0\r\n",
        "dictionary_chapter = {}\r\n",
        "for a, b in enumerate(index_new_chapter):\r\n",
        "    dictionary_chapter[a] = data[c:b]\r\n",
        "    c = b\r\n",
        "dictionary_chapter[170] = data[299557:300079] \r\n",
        "\r\n",
        "\r\n",
        "def calculator_tf(target_word, target_chapter):\r\n",
        "      \r\n",
        "    target_word_counter = 0\r\n",
        "    for i in dictionary_chapter[target_chapter]:\r\n",
        "        if i == target_word:\r\n",
        "            target_word_counter +=1\r\n",
        "    \r\n",
        "    total_words_in_chapter = len(dictionary_chapter[target_chapter])-1\r\n",
        "    return target_word_counter / total_words_in_chapter\r\n",
        "\r\n",
        "# Count intances of target_word\r\n",
        "def count_instances(target_word):\r\n",
        "    count_1 = 0\r\n",
        "    i = 1\r\n",
        "    while i < len(dictionary_chapter):\r\n",
        "        if target_word in dictionary_chapter[i]:\r\n",
        "            count_1 += 1\r\n",
        "        i +=1\r\n",
        "    return count_1/171\r\n",
        "\r\n",
        "count_tartget_word_num = count_instances(target_word)\r\n",
        "\r\n",
        "def contrast(tf, df):\r\n",
        "    return math.log(1+tf) * math.log(1/df)\r\n",
        "\r\n",
        "tf_x_df = contrast(calculator_tf(target_word, target_chapter),count_instances(target_word))\r\n",
        "\r\n",
        "#Проверка результата\r\n",
        "target_word = 'анна'\r\n",
        "target_chapter = 4\r\n",
        "print(tf_x_df)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.011031063403921838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt-CBNAOJOCU"
      },
      "source": [
        "# Задание 5\r\n",
        "\r\n",
        "Теперь, когда мы умеем вычислять `tf*idf` для каждого слова в главе, мы можем найти те слова, которые являются самыми «контрастными» для данной главы, то есть они могут являться в своём роде заголовком для главы.\r\n",
        "\r\n",
        "**Задача:** напишите код, который выведет на экран через пробел три слова, имеющие самое высокое значение `tf*idf` в заданной главе `target_chapter` в порядке убывания `tf*idf`.\r\n",
        "\r\n",
        "Например, для главы `4` ответом будет:\r\n",
        "`павловна анна прядильной`\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL50FDHpIsba",
        "outputId": "2608822a-8727-47f0-f63b-983330c38b80"
      },
      "source": [
        "from math import log\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "def read_data():\r\n",
        "    data = open('data/war_peace_processed.txt', 'rt').read()\r\n",
        "    return data.split('\\n')\r\n",
        "\r\n",
        "data = read_data()\r\n",
        "n = 0\r\n",
        "words = 0\r\n",
        "count = Counter(data)['[new chapter]'] + 1\r\n",
        "chap_words = {}\r\n",
        "uniq_chapters = {}\r\n",
        "\r\n",
        "for i in data:\r\n",
        "    uniq_chapters.setdefault(n, set()).add(i)\r\n",
        "    if i == '[new chapter]':\r\n",
        "        n += 1\r\n",
        "    if n == target_chapter:\r\n",
        "        words += 1\r\n",
        "        chap_words[i] = chap_words.get(i, 0) + 1\r\n",
        "uniq_words_count = {}\r\n",
        "\r\n",
        "for i in chap_words.keys():\r\n",
        "    for k in uniq_chapters:\r\n",
        "        if i in uniq_chapters[k]:\r\n",
        "            uniq_words_count[i] = uniq_words_count.get(i, 0) + 1\r\n",
        "            \r\n",
        "for i in uniq_words_count:\r\n",
        "    uniq_words_count[i] = uniq_words_count[i] /count\r\n",
        "\r\n",
        "tf_idf = [(i, log(1 + chap_words[i] / words) * log(1 / (uniq_words_count[i]))) for i in uniq_words_count]\r\n",
        "print(*[j[0] for j in sorted([i for i in tf_idf], key=lambda x: x[1])[-3:][::-1]])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "павловна анна прядильной\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oagroZ36Jx5S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}